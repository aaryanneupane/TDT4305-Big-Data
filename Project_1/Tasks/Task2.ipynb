{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83045409-70a5-4010-b1cf-9db8d98f2bad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059a269d-8f13-494e-84e5-9da5d873047b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Anne Torgersen\n",
    "\n",
    "Aaryan Neupane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52939e3d-35c7-4767-a59e-ba75f716e952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499e46f5-12a4-46de-a212-90f2e936461e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "import unittest\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8c68e0-6bda-40a6-8588-381e51dc0a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.parquet(table_name)\n",
    "\n",
    "users_df = load_df(\"/user/hive/warehouse/users\")\n",
    "comments_df = load_df(\"/user/hive/warehouse/comments\")\n",
    "posts_df = load_df(\"/user/hive/warehouse/posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2274f0-1bd8-4812-83d2-f793587e9548",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: implenenting two helper functions\n",
    "Impelment these two functions:\n",
    "1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n",
    "2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n",
    "\n",
    "Note that the result of a Spark SQL query is itself a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fce811-7dd2-4602-a243-18a36754c302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    result_df = spark.sql(query)\n",
    "    first_row = result_df.first()\n",
    "    return first_row[0]\n",
    "\n",
    "def run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "    result_df = spark.sql(query)\n",
    "    first_row = result_df.first()\n",
    "    return first_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a021cce-880b-42b1-a09d-6d2c2222124e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de10aba-7e77-4baa-af35-2b4e37916071",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: writing a few queries\n",
    "Write the following queries in SQL to be executed by Spark in the next cell.\n",
    "\n",
    "1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n",
    "2. 'q2': find the number users\n",
    "3. 'q3': find the 'Id' of the user who posted most number of answers\n",
    "4. 'q4': find the number of questions\n",
    "5. 'q5': find the display name of the user who posted most number of comments\n",
    "\n",
    "Note that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3273ef7-7f4d-4b5d-bcf7-9935c952e26d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q1 = '''\n",
    "SELECT * \n",
    "FROM df \n",
    "ORDER BY CreationDate\n",
    "DESC limit 1\n",
    "'''\n",
    "\n",
    "q2 = '''\n",
    "SELECT count(*) \n",
    "FROM df '''\n",
    "\n",
    "q3 = '''\n",
    "SELECT OwnerUserId FROM df\n",
    "GROUP BY OwnerUserId\n",
    "ORDER BY count(CASE WHEN PostTypeId = 2 THEN 1 ELSE NULL END)\n",
    "DESC limit 1\n",
    "'''\n",
    "\n",
    "q4 = '''\n",
    "SELECT count(*) \n",
    "FROM df \n",
    "WHERE PostTypeId == 1\n",
    "'''\n",
    "\n",
    "q5 = '''\n",
    "SELECT df1.DisplayName \n",
    "FROM df1 INNER JOIN df2 \n",
    "ON df1.Id = df2.UserId \n",
    "GROUP BY df1.Id, df1.DisplayName \n",
    "ORDER BY count(*) \n",
    "DESC limit 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46400bac-c11c-4bee-81a8-67d42e3ca968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementations by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ca3fb9-427c-425d-b334-0df9c208a21b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 11.289s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 11.289s\n",
      "\n",
      "OK\n",
      "Out[7]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"
     ]
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask2(unittest.TestCase):\n",
    "    def test_q1(self):\n",
    "        # find the id of the most recent post\n",
    "        r = run_query(q1, posts_df)\n",
    "        self.assertEqual(r, 95045)\n",
    "\n",
    "    def test_q2(self):\n",
    "        # find the number of the users\n",
    "        r = run_query(q2, users_df)\n",
    "        self.assertEqual(r, 91616)\n",
    "\n",
    "    def test_q3(self):\n",
    "        # find the user id of the user who posted most number of answers\n",
    "        r = run_query(q3, posts_df)\n",
    "        self.assertEqual(r, 64377)\n",
    "\n",
    "    def test_q4(self):\n",
    "        # find the number of questions\n",
    "        r = run_query(q4, posts_df)\n",
    "        self.assertEqual(r, 28950)\n",
    "\n",
    "    def test_q5(self):\n",
    "        # find the display name of the user who posted most number of comments\n",
    "        r = run_query2(q5, users_df, comments_df)\n",
    "        self.assertEqual(r, \"Neil Slater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8a2e0c-7e72-410b-b385-00503c0bc136",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n",
    "2. When do you suggest using RDDs instead of using DataFrames?\n",
    "3. What is the main benefit of using DataSets instead of DataFrames?\n",
    "\n",
    "Write your answers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c33ca21-1672-4c0c-a876-f73cbb8f65b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'?\n",
    "\n",
    "- These terms refers to different data structures of Spark. All of these concepts are used for distributed processing of data, and they are all in use today. The RDDs came first when Spark launched in 2011, and DataFrames and Datasets have arrived later. The RDD is the fundamental data structure of spark, while DataFrames and DataSets were developed to provide more structured, optimized and user friendly ways of working with distributed data. The data in DataFrames is organized into named columns, and in difference to RDDs allows developers to debug during runtime.  Datasets are an extension of DataFrames that provide a type-safe, object-oriented programming interface. The coding is similar to RDDs, but faster. \n",
    "\n",
    "2. When do you suggest using RDDs instead of using DataFrames?\n",
    "\n",
    "- RDDs could be a better choice if your data processing involves complex operations requiring low-level control. Some operations are also specific for RDDs, and canâ€™t be executed in the DataFrame format. DataFrames is beneficial when working with structured data, but if the data lacks structure, RDDs could be more suitable. \n",
    "\n",
    "3. What is the main benefit of using DataSets instead of DataFrames?\n",
    "\n",
    "- The main benefit of using DataSets instead of DataFrames is the advantages that comes with ensuring type-safety. This can lead to improved code reliability and performance."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
